{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd\nimport cv2,math,gc\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torch.nn import Parameter\nimport torch.optim as optim\n\n!pip install \"../input/efficient-net/dist/efficientnet_pytorch-0.7.0.tar\"\nfrom efficientnet_pytorch import EfficientNet\n\n!pip install \"../input/faissgpuwheel/faiss_gpu-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl\"\nimport faiss\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n\nimport warnings\nwarnings.simplefilter('ignore')\n    \ntorch.backends.cudnn.benchmark = True\nfrom transformers import (BertTokenizer, BertModel,\n                          DistilBertTokenizer, DistilBertModel)\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm.autonotebook import tqdm","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/efficient-net/dist/efficientnet_pytorch-0.7.0.tar\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch==0.7.0) (1.7.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.0) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.0) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.0) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.0) (1.19.5)\n\u001b[33mDEPRECATION: Source distribution is being reinstalled despite an installed package having the same name and version as the installed package. pip 21.1 will remove support for this functionality. A possible replacement is use --force-reinstall. You can find discussion regarding this at https://github.com/pypa/pip/issues/8711.\u001b[0m\nBuilding wheels for collected packages: efficientnet-pytorch\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.0-py3-none-any.whl size=16033 sha256=ca59db0a5d4da10e47b08a082320dbaa37994b6db10151a59da76160b0febc62\n  Stored in directory: /root/.cache/pip/wheels/af/8c/80/1bf8cc2fa471c320978f34c5290675daaa96446e1b9ba45555\nSuccessfully built efficientnet-pytorch\nInstalling collected packages: efficientnet-pytorch\n  Attempting uninstall: efficientnet-pytorch\n    Found existing installation: efficientnet-pytorch 0.7.0\n    Uninstalling efficientnet-pytorch-0.7.0:\n      Successfully uninstalled efficientnet-pytorch-0.7.0\nSuccessfully installed efficientnet-pytorch-0.7.0\nProcessing /kaggle/input/faissgpuwheel/faiss_gpu-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl\nfaiss-gpu is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n","output_type":"stream"}]},{"cell_type":"code","source":"TRAIN = True","metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\nlabels = list(set(df.label_group.values))\nlabels.sort()\nlabels_length = len(labels)\nsingle_fold_length = labels_length//5 + 1\nrank = dict()\nfor i, label in enumerate(labels):\n    rank[label] = i\ndf['rank'] = df.label_group.map(rank)\ndf['fold'] = df['rank'].apply(lambda x: x//single_fold_length)\ndf.head()","metadata":{"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n\n                                               title  label_group   rank  fold  \n0                          Paper Bag Victoria Secret    249114794    666     0  \n1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   7572     3  \n2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   6172     2  \n3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188  10509     4  \n4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   9425     4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>rank</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>666</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>7572</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>6172</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>10509</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n      <td>9425</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class cfg:\n    img_size = (380,380)\n    feavec_num1 = 512\n    feavec_num2 = 768\n    fea_norm = 64\n    margin = 0.35\n    batch = 16\n    mname = 'efficientnet-b3'\n    clsize = 8812\n    lr = 0.001\n    momentum = 0.9\n    weight_decay = 0.0005\n    log_interval = 1000\n    epochs = 10\n    wpath = '../input/my-weight/efficientnet-b3_eff_bert_arcface_epoch_10.pt'","metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    DistilBERT = True # if set to False, BERT model will be used\n    bert_hidden_size = 768\n    \n    batch_size = 64\n    epochs = 100\n    num_workers = 4\n    learning_rate = 1e-5 \n    scheduler = \"ReduceLROnPlateau\"\n    step = 'epoch'\n    patience = 2\n    factor = 0.8\n    dropout = 0.5\n    model_path = \"/kaggle/working\"\n    max_length = 30\n    model_save_name = \"bert_model.pt\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\ndf['target'] = df.label_group.map(tmp)\ndf['target'] = df['target'].apply(lambda x: ' '.join(x))\ndf_cu = cudf.DataFrame(df)\n\nprint('df shape is', df.shape )\ndf.head()","metadata":{"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"df shape is (34250, 8)\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n\n                                               title  label_group   rank  \\\n0                          Paper Bag Victoria Secret    249114794    666   \n1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   7572   \n2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   6172   \n3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188  10509   \n4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   9425   \n\n   fold                             target  \n0     0   train_129225211 train_2278313361  \n1     3  train_3386243561 train_3423213080  \n2     2  train_2288590299 train_3803689425  \n3     4  train_2406599165 train_3342059966  \n4     4   train_3369186413 train_921438619  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>rank</th>\n      <th>fold</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>666</td>\n      <td>0</td>\n      <td>train_129225211 train_2278313361</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>7572</td>\n      <td>3</td>\n      <td>train_3386243561 train_3423213080</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>6172</td>\n      <td>2</td>\n      <td>train_2288590299 train_3803689425</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>10509</td>\n      <td>4</td>\n      <td>train_2406599165 train_3342059966</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n      <td>9425</td>\n      <td>4</td>\n      <td>train_3369186413 train_921438619</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df1 = df[df.fold!=4]\ndf1_val = df[df.fold==4]\ndf1_val['label'] = 1\nranks = list(set(df1[\"rank\"].values))\nranks.sort()\nranks_length = len(ranks)\nprint(ranks_length)\nlabel = dict()\nfor i, rank in enumerate(ranks):\n    label[rank] = i\ndf1['label'] = df1[\"rank\"].map(label)\ndf1.head()","metadata":{"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"8812\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n5  train_2464356923  0013e7355ffc5ff8fb1ccad3e42d92fe.jpg  bbd097a7870f4a50   \n6  train_1802986387  00144a49c56599d45354a1c28104c039.jpg  f815c9bb833ab4c8   \n\n                                               title  label_group  rank  fold  \\\n0                          Paper Bag Victoria Secret    249114794   666     0   \n1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045  7572     3   \n2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891  6172     2   \n5  CELANA WANITA  (BB 45-84 KG)Harem wanita (bisa...   2660605217  6836     3   \n6                           Jubah anak size 1-12 thn   1835033137  4687     2   \n\n                                              target  label  \n0                   train_129225211 train_2278313361    666  \n1                  train_3386243561 train_3423213080   7572  \n2                  train_2288590299 train_3803689425   6172  \n5  train_2464356923 train_2753295474 train_305884580   6836  \n6  train_1802986387 train_1396161074 train_713073...   4687  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>rank</th>\n      <th>fold</th>\n      <th>target</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>666</td>\n      <td>0</td>\n      <td>train_129225211 train_2278313361</td>\n      <td>666</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>7572</td>\n      <td>3</td>\n      <td>train_3386243561 train_3423213080</td>\n      <td>7572</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>6172</td>\n      <td>2</td>\n      <td>train_2288590299 train_3803689425</td>\n      <td>6172</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>train_2464356923</td>\n      <td>0013e7355ffc5ff8fb1ccad3e42d92fe.jpg</td>\n      <td>bbd097a7870f4a50</td>\n      <td>CELANA WANITA  (BB 45-84 KG)Harem wanita (bisa...</td>\n      <td>2660605217</td>\n      <td>6836</td>\n      <td>3</td>\n      <td>train_2464356923 train_2753295474 train_305884580</td>\n      <td>6836</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>train_1802986387</td>\n      <td>00144a49c56599d45354a1c28104c039.jpg</td>\n      <td>f815c9bb833ab4c8</td>\n      <td>Jubah anak size 1-12 thn</td>\n      <td>1835033137</td>\n      <td>4687</td>\n      <td>2</td>\n      <td>train_1802986387 train_1396161074 train_713073...</td>\n      <td>4687</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"if CFG.DistilBERT:\n    model_name='cahya/distilbert-base-indonesian'\n    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n    bert_model = DistilBertModel.from_pretrained(model_name)\nelse:\n    model_name='cahya/bert-base-indonesian-522M'\n    tokenizer = BertTokenizer.from_pretrained(model_name)\n    bert_model = BertModel.from_pretrained(model_name)","metadata":{"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, s=30.0, m=0.30, easy_margin=False):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        one_hot = torch.zeros(cosine.size(), device=device)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output  #need softmax then\n\n\nclass Model(nn.Module):\n    def __init__(self,name,clustersize,feavec, bert_model, last_hidden_size=CFG.bert_hidden_size):\n        super(Model, self).__init__()\n        self.eff = EfficientNet.from_pretrained(name)\n        self.bert_model = bert_model\n        self.out = nn.Linear(1000+last_hidden_size,feavec)\n        self.margin = ArcMarginProduct(in_features=feavec, \n                                       out_features = clustersize, \n                                       s=cfg.fea_norm, \n                                       m=cfg.margin)      \n    \n    def get_bert_features(self, batch):\n        output = self.bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n        last_hidden_state = output.last_hidden_state # shape: (batch_size, seq_length, bert_hidden_dim)\n        CLS_token_state = last_hidden_state[:, 0, :] # obtaining CLS token state which is the first token.\n        return CLS_token_state\n\n    def forward(self, batch, labels=None):\n        x1 = self.eff(batch['image'])\n        x2 = self.get_bert_features(batch)\n        \n        x = torch.hstack((x1, x2))\n        x = self.out(x)\n        if labels is not None:\n            return self.margin(x,labels)\n        return F.normalize(x,dim=1)","metadata":{"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model = Model(name=cfg.mname,clustersize=cfg.clsize, feavec=512, bert_model=bert_model).to(device)\nmodel.load_state_dict(torch.load(cfg.wpath, map_location=device))\noptimizer = optim.SGD(model.parameters(), lr=cfg.lr, momentum=cfg.momentum, weight_decay=cfg.weight_decay)","metadata":{"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b3\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_image(file_name):\n    if TRAIN:\n        file_path = f'/kaggle/input/shopee-product-matching/train_images/{file_name}'\n    else:\n        file_path = f'/kaggle/input/shopee-product-matching/test_images/{file_name}'\n    img = cv2.imread(file_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, cfg.img_size)\n    tensor_img = torch.tensor(img)\n    tensor_img = tensor_img.permute(( 2, 0, 1)).float()/255.0\n    return tensor_img","metadata":{"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, mode=\"train\", max_length=None):\n        self.dataframe = dataframe\n        if mode != \"test\":\n            self.targets = dataframe['label'].values\n        texts = list(dataframe['title'].apply(lambda o: str(o)).values)\n        self.encodings = tokenizer(texts, \n                                   padding=True, \n                                   truncation=True, \n                                   max_length=max_length)\n        self.mode = mode\n        self.img = dataframe.image.values\n        \n    def __getitem__(self, idx):\n        # putting each tensor in front of the corresponding key from the tokenizer\n        # HuggingFace tokenizers give you whatever you need to feed to the corresponding model\n        item = {key: torch.tensor(values[idx]) for key, values in self.encodings.items()}\n        # when testing, there are no targets so we won't do the following\n        if self.mode != \"test\":\n            item['labels'] = torch.tensor(self.targets[idx]).long()\n        img = self.img[idx]\n        img = load_image(img)\n        item['image'] = img\n        return item\n    \n    def __len__(self):\n        return len(self.dataframe)","metadata":{"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train_dataset = MyDataset(df1, tokenizer, mode='train', max_length=CFG.max_length)\ntrain_loader = DataLoader(train_dataset,\n                    batch_size=cfg.batch,\n                    shuffle=False,\n                    num_workers=2,\n                    pin_memory=True,\n                    drop_last=False)\nval_dataset = MyDataset(df1_val, tokenizer, mode='train', max_length=CFG.max_length)\nval_loader = DataLoader(val_dataset,\n                    batch_size=cfg.batch,\n                    shuffle=False,\n                    num_workers=2,\n                    pin_memory=True,\n                    drop_last=False)","metadata":{"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def train(epoch):\n    model.train()\n    epoch_loss = 0\n    for batch_idx, batch in enumerate(train_loader):\n        batch = {k: v.to(CFG.device) for k, v in batch.items()}\n        images = batch['image']\n        label = batch['labels']\n        optimizer.zero_grad()\n        output = model(batch, label)\n        loss = nn.CrossEntropyLoss()(output, label)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % cfg.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(images), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n        epoch_loss = epoch_loss + loss.item()\n    return epoch_loss/len(train_loader)","metadata":{"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def all_embeddings(df):\n    dataset = MyDataset(df, tokenizer, mode='train', max_length=CFG.max_length)\n    loader = DataLoader(dataset,\n                        batch_size=cfg.batch,\n                        shuffle=False,\n                        num_workers=2,\n                        pin_memory=True,\n                        drop_last=False)\n    \n    model.eval()\n    print('start collection')\n    feavec = 512\n    embedded1 = np.empty((0,feavec),dtype='float32')\n    with torch.no_grad():\n        for idx,batch in enumerate(loader):\n            batch = {k: v.to(CFG.device) for k, v in batch.items()}\n            outputs = model(batch)\n            embedded1 = np.append(embedded1, outputs.cpu().detach().numpy(),axis=0)\n\n            if idx%100==0:\n                print(idx,len(loader))\n                print(embedded1.shape)\n    return embedded1","metadata":{"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1\n\ndef predict_img(df,embeddings,topk=50,threshold=0.63):\n    N,D = embeddings.shape\n    cpu_index = faiss.IndexFlatL2(D)\n    gpu_index = faiss.index_cpu_to_all_gpus(cpu_index)\n    gpu_index.add(embeddings)\n    cluster_distance,cluster_index = gpu_index.search(x=embeddings, k=topk)\n    \n    df['pred_images'] = ''\n    pred = []\n    for k in range(embeddings.shape[0]):\n        idx = np.where(cluster_distance[k,] < threshold)[0]\n        ids = cluster_index[k,idx]\n        #posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n        posting_ids = df['posting_id'].iloc[ids].values\n        pred.append(posting_ids)\n    df['pred_images'] = pred\n    \n    df['pred_imgonly'] = df.pred_images.apply(lambda x: ' '.join(x))\n    df['f1_img'] = f1_score(df['target'], df['pred_imgonly'])\n    score = df['f1_img'].mean()\n    #print(f'Our f1 score for threshold {threshold} is {score}')\n    return score","metadata":{"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"loss_list = []\ntrain_f1_list = []\nval_f1_list = []\nfor epoch in range(1,cfg.epochs + 1):\n    loss_list.append(train(epoch))\n    \n    embeddings_train = all_embeddings(df1)\n    embeddings_val = all_embeddings(df1_val)\n    train_f1_score = predict_img(df1,embeddings_train, topk=50,threshold=0.88)\n    val_f1_score = predict_img(df1_val,embeddings_val, topk=50,threshold=0.88)\n    \n    train_f1_list.append(train_f1_score)\n    val_f1_list.append(val_f1_score)\n    \n    torch.save(model.state_dict(), \"{}_eff_bert_arcface_epoch_{}.pt\".format(cfg.mname, epoch))\n    print(\"train_f1_score:\", train_f1_score)\n    print(\"val_f1_score:\", val_f1_score)\n\nloss_list = np.array(loss_list)\ntrain_f1_list = np.array(train_f1_list)\nval_f1_list = np.array(val_f1_list)\n\nnp.save('image_only_train_loss.npy', loss_list)\nnp.save('image_only_train_f1.npy', train_f1_list)\nnp.save('image_only_val_f1.npy', val_f1_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(loss_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(train_f1_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(val_f1_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for thre in np.arange(70, 120)*0.01:\n    val_f1_score = predict_img(df1_val,embeddings_val, topk=50,threshold=thre)\n    print(thre, ':', val_f1_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.arange(70, 100)*0.01","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_val = all_embeddings(df1_val)","metadata":{"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"start collection\n0 425\n(16, 512)\n100 425\n(1616, 512)\n200 425\n(3216, 512)\n300 425\n(4816, 512)\n400 425\n(6416, 512)\n","output_type":"stream"}]},{"cell_type":"code","source":"val_f1_score = predict_img(df1_val,embeddings_val, topk=50,threshold=0.6)\nprint( ':', val_f1_score)","metadata":{"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":": 0.7156906746387467\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}