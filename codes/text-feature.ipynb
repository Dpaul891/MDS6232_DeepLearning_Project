{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch_transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_transformers import  BertModel, BertConfig,BertTokenizer\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport gc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextNet(nn.Module):\n    def __init__(self,  code_length): #code_length为fc映射到的维度大小\n        super(TextNet, self).__init__()\n\n        modelConfig = BertConfig.from_pretrained('bert-base-uncased')\n        self.textExtractor = BertModel.from_pretrained('bert-base-uncased', config=modelConfig)\n        embedding_dim = self.textExtractor.config.hidden_size\n\n        self.fc = nn.Linear(embedding_dim, code_length)\n        self.tanh = torch.nn.Tanh()\n\n    def forward(self, tokens, segments, input_masks):\n        output=self.textExtractor(tokens, token_type_ids=segments,\n                                         attention_mask=input_masks)\n        text_embeddings = output[0][:, 0, :]\n        #output[0](batch size, sequence length, model hidden dimension)\n        \n        #return text_embeddings\n        features = self.fc(text_embeddings)\n        del output, text_embeddings\n        gc.collect()\n        features=self.tanh(features)\n        return features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"textNet = TextNet(code_length=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## dataset","metadata":{}},{"cell_type":"code","source":"titles = pd.read_csv(\"../input/shopee-product-matching/train.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = [\"[CLS] \"+unit+\" [SEP]\" for unit in titles.title]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ntokens, segments, input_masks = [], [], []\nfor text in texts:\n    tokenized_text = tokenizer.tokenize(text) #用tokenizer对句子分词\n    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)#索引列表\n    tokens.append(indexed_tokens)\n    segments.append([0] * len(indexed_tokens))\n    input_masks.append([1] * len(indexed_tokens))\n\nmax_len = max([len(single) for single in tokens]) #最大的句子长度\n\nfor j in range(len(tokens)):\n    padding = [0] * (max_len - len(tokens[j]))\n    tokens[j] += padding\n    segments[j] += padding\n    input_masks[j] += padding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens_tensor = torch.tensor(tokens)\nsegments_tensors = torch.tensor(segments)\ninput_masks_tensors = torch.tensor(input_masks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_hashCodes = textNet(tokens_tensor , segments_tensors , input_masks_tensors )\ntext_hashCodes.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}